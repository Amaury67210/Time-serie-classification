{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.utils\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count":95,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"zTGqTbYJRxU89LMhviBfEt",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "## import des données \n",
    "df = pd.read_csv(\"\/data\/notebook_files\/oil_wells_data.csv\")"
   ],
   "execution_count":96,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"9p7agqBhtdXbIwf9eEtsjH",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## On effectue ensuite la preparation des données telle que décrite dans le premier rapport"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"uHQZQPIVloqdCm7kuWPV4I",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#premier observations des données\n",
    "df.info()"
   ],
   "execution_count":97,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26999 entries, 0 to 26998\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   timestamp   26999 non-null  object \n",
      " 1   P-PDG       26999 non-null  float64\n",
      " 2   P-TPT       26999 non-null  float64\n",
      " 3   T-TPT       26999 non-null  float64\n",
      " 4   P-MON-CKP   26999 non-null  float64\n",
      " 5   T-JUS-CKP   26999 non-null  float64\n",
      " 6   P-JUS-CKGL  0 non-null      float64\n",
      " 7   T-JUS-CKGL  0 non-null      float64\n",
      " 8   QGL         0 non-null      float64\n",
      " 9   class       26999 non-null  int64  \n",
      "dtypes: float64(8), int64(1), object(1)\n",
      "memory usage: 2.1+ MB\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"t5Y5WegJsuaxXY1vqjKYVJ",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "df.head()"
   ],
   "execution_count":98,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>timestamp<\/th>\n",
       "      <th>P-PDG<\/th>\n",
       "      <th>P-TPT<\/th>\n",
       "      <th>T-TPT<\/th>\n",
       "      <th>P-MON-CKP<\/th>\n",
       "      <th>T-JUS-CKP<\/th>\n",
       "      <th>P-JUS-CKGL<\/th>\n",
       "      <th>T-JUS-CKGL<\/th>\n",
       "      <th>QGL<\/th>\n",
       "      <th>class<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>2018-05-17 22:56:06.000000<\/td>\n",
       "      <td>27897490.0<\/td>\n",
       "      <td>19824230.0<\/td>\n",
       "      <td>125.6859<\/td>\n",
       "      <td>4059666.0<\/td>\n",
       "      <td>97.55283<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1<\/th>\n",
       "      <td>2018-05-17 22:56:07.000000<\/td>\n",
       "      <td>27897450.0<\/td>\n",
       "      <td>19824230.0<\/td>\n",
       "      <td>125.6859<\/td>\n",
       "      <td>4059666.0<\/td>\n",
       "      <td>97.55283<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2<\/th>\n",
       "      <td>2018-05-17 22:56:08.000000<\/td>\n",
       "      <td>27897360.0<\/td>\n",
       "      <td>19824230.0<\/td>\n",
       "      <td>125.6859<\/td>\n",
       "      <td>4059666.0<\/td>\n",
       "      <td>97.55283<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>3<\/th>\n",
       "      <td>2018-05-17 22:56:09.000000<\/td>\n",
       "      <td>27897430.0<\/td>\n",
       "      <td>19824230.0<\/td>\n",
       "      <td>125.6859<\/td>\n",
       "      <td>4059666.0<\/td>\n",
       "      <td>97.55282<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>4<\/th>\n",
       "      <td>2018-05-17 22:56:10.000000<\/td>\n",
       "      <td>27897500.0<\/td>\n",
       "      <td>19824230.0<\/td>\n",
       "      <td>125.6859<\/td>\n",
       "      <td>4059666.0<\/td>\n",
       "      <td>97.55282<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>NaN<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"SU6oexFusc59bMjVMSbZMz",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "On remarque que 3 colonnes sont vides, on les supprime donc. On enlève également le timestemp car les données sont déjà dans le bon ordre chronologique, l'information du temps est donc conservée."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"tlhEn8Cj4QrqQC8vIIJinB",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "df.pop('P-JUS-CKGL')\n",
    "df.pop('T-JUS-CKGL')\n",
    "df.pop('QGL')\n",
    "df.pop('timestamp')"
   ],
   "execution_count":99,
   "outputs":[
    {
     "data":{
      "text\/html":[
       
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"CzxbDx7R3ImYhuY6Iqw236",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#on change la représentation numérique des classes 108 et 8 pour avoir un one-hot de \n",
    "#3 classes et non pas 108.\n",
    "df.loc[df['class'] == 108, 'class'] = 1\n",
    "df.loc[df['class'] == 8, 'class'] = 2"
   ],
   "execution_count":100,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"dDXnipQE3fHkbQKfrYNL76",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#on met les 'class' dans Y en utilisant un encodage one-hot \n",
    "#et on enleve les class de notre série temporelle\n",
    "Y = tensorflow.keras.utils.to_categorical(df['class'],3)\n",
    "Y = pd.DataFrame(Y)\n",
    "df.pop('class')"
   ],
   "execution_count":101,
   "outputs":[
    {
     "data":{
      "text\/html":[
       
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"sb2dykSXTDLgfRCQJ2XKqv",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Séquençage des données \n",
    "Dans cette section je programme une fonction qui va servir à tranformer nos données en séquences de taille fixe. Après plusieurs essais je trouve que la longueur de séquence idéale se trouve certainement dans l'intervalle 20,45 qui est un intervalle assez large. Pour des séquences de taille dans cet intervalle on trouve de bons résultats qui sont assez similaires. Personnellement, j'ai fait le choix de travailler avec des séquences de taille **30**."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"akcIqby8v6lXt2qkgqatMt",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#transformé les données en séquences \n",
    "def create_sequences(input_data: pd.DataFrame, target_column: pd.DataFrame, seq_len) :\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    data_size = len(input_data)\n",
    "    #+1 car on veux toutes les séquences possible, on utilise tqdm pour visualiser l'avancée de la boucle.\n",
    "    for i in tqdm(range(data_size - seq_len + 1)) :\n",
    "        sequence = input_data[i:i+seq_len]\n",
    "        label_sequence = target_column[i:i+seq_len]\n",
    "        label = 0\n",
    "        #calcul du label correspondant -> on prend le max car on doit faire un choix de label parmis tous les labels de la séquence\n",
    "        #ici on choisit ça car on s'intéresse à la détection d'anomalie, étant donnée qu'on veut les détecter le plus rapidement \n",
    "        #possible, on associe le label maximal car apres modifications des labels de class, ils valent 0,1 et 2. Leurs \"urgence\"\n",
    "        # est décroissante càd urgence(0) < urgence(1) < urgence(2)\n",
    "        if(label_sequence.max()[2] == 1) :\n",
    "            label = 2\n",
    "        elif(label_sequence.max()[1] == 1) :\n",
    "            label = 1\n",
    "        \n",
    "        sequences.append(sequence)\n",
    "        labels.append(label)\n",
    "\n",
    "    return sequences,labels"
   ],
   "execution_count":102,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"WmUP8YSKqvHB27vLKGpxAy",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "sequence_length = 30\n",
    "x_seq, y_seq = create_sequences(df,Y,sequence_length)"
   ],
   "execution_count":103,
   "outputs":[
    {
     "data":{
      "application\/vnd.jupyter.widget-view+json":{
       "version_major":2,
       "version_minor":0,
       "model_id":"8a582d357493472481ed22edf1dd2c6e"
      }
     },
     "metadata":{
      "application\/vnd.jupyter.widget-view+json":{
       "datalore":{
        "widget_id":"OSAtStDFB1MLWCLmNqrBwy"
       }
      }
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"XdqWlwIiJhaxwQZRYzQXTz",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "On transforme les séquences en array numpy pour pouvoir s'assurer qu'elles soient de la bonne forme, c'est à dire : ( nombre de séquences $\\times $ longueur des séquences $\\times$ nombre de variables). De plus on doit re-effectuer un encodage one-hot de nos labels."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"ASYVMrH4aUvWyUrehRJiqs",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "x_seq = np.array(x_seq)\n",
    "y_seq = tensorflow.keras.utils.to_categorical(y_seq,3)\n",
    "print(x_seq.shape)\n",
    "print(y_seq.shape)"
   ],
   "execution_count":104,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "(26970, 30, 5)\n",
      "(26970, 3)\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"2VnvSOqH89dvzrNrauoKHD",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Séparation des jeux de données\n",
    "Maintenant que nous disposons de nos séquences, avant d'aller plus loin dans les traitements de ces dernières, nous devons les séparer en jeu d'entrainement et jeu de test. Le jeu de test va nous permettre de voir si notre model généralise bien sur des données qui n'ont pas servi à l'entrainement, cela permet de se rendre contre du sur-apprentisage mais également de la capacité d'un modèle à bien généraliser ou non. \n",
    "\n",
    "Pour ce faire, on utilise la fonction `train_test_split()` proposée par la librairie *scikit-learn*."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"7Pa2IrG4su6hWjvS0J7Etu",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "x_train, x_test, y_train, y_test = train_test_split(x_seq,y_seq)"
   ],
   "execution_count":105,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"OOBQwo4sGw7PiBDCsRNlJn",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ],
   "execution_count":106,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"h9kALj1XbSrzXoKOnDSH23",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Standardisation des données \n",
    "On standardise maintenant les données pour conserver l'inertie des données. De plus cela va permettre de lutter contre la disparition et l'explosion du gradient. Enfin, cela va empêcher au modèle de donner potentiellement trop d'importance aux variables prenant de grandes valeurs (Les échelles de valeurs selon les attributs étant très variées).\n",
    "\n",
    "On effectue cette oppération uniquement après avoir séparé le jeu de données en 2 jeux car la standardisation est initialisée avec `.fit()` cela va permettre de calculer la moyenne et l'écart type du jeu de données que l'on passe en paramètre. On veut donc uniquement appliquer ce fit aux données d'entrainements, en revanche par la suite nous pourront utiliser notre standardisation avec la fonction `.transform()` pour appliquer cette dernière sur les données de test avec la moyenne et l'écart-type qui aura été calculé sur le jeu d'entrainement."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"gXe5DprIa83AYg6O3ChyxH",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#\"initialisation\" et application de la standardisation avec les données d'entrainement\n",
    "#le scaler n'accepte que les données 2D, il faut donc changer la dimension des données puis revenir cette dimension \n",
    "scaler = StandardScaler()\n",
    "shape_dim1, shape_dim2, shape_dim3 = x_train.shape\n",
    "x_train = np.reshape(x_train, newshape=(-1, shape_dim3))\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_train = np.reshape(x_train, newshape=(shape_dim1, shape_dim2, shape_dim3))"
   ],
   "execution_count":107,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"1OkHVCX9yeBFTbjRyhI5iW",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# On applique maintenant la standardisation des données aux données de test :\n",
    "shape_dim1, shape_dim2, shape_dim3 = x_test.shape\n",
    "x_test = np.reshape(x_test, newshape=(-1, shape_dim3))\n",
    "x_test = scaler.transform(x_test)\n",
    "x_test = np.reshape(x_test, newshape=(shape_dim1, shape_dim2, shape_dim3))"
   ],
   "execution_count":108,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"dOOoqS1wa6dwlPkABwo7PS",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Définition du modèle\n",
    "\n",
    "Pour ce qui est du CNN j'ai choisi de m'inspirer de l'architecture FCN (fully convolutionnal network) de cet [article](https:\/\/arxiv.org\/pdf\/1611.06455.pdf) sur les réseaux profonds pour la classification de séries temporelles. Néanmoins j'ai trouvé cette architecture trop \"profonde\" pour notre problème bien qu'elle n'ai que 3 couches. J'ai donc dans un premier temps implémenté le réseaux tel quel en retirant simplement la dernière couche. On obtient donc un modèle de la forme : conv1D > BatchNorm > ReLU fois 2, puis cette sortie passait dans une couche \"GlobalAveragePooling\" pour finir dans un classifier à 3 sorties comme nous disposons de 3 classes (avec comme fonction non-linéaire la fonction softmax).\n",
    "\n",
    "On remarque que cette architecture ne réduit pas beaucoup la dimension de l'entrée lors des couches de convolution car elle n'utilise pas de pooling, à la place on utilise une batch normalization. Cette batch normalization permet ([d'après cet article](https:\/\/arxiv.org\/abs\/1502.03167)) d'accélérer l'entrainement. En effet, elle vise à re-centrer et réduire les sorties des couches de convolutions, de fait la distribution des entrées ne changent pas d'une couche à une autre. Cela va nous permettre d'utiliser des pas d'apprentissage plus grands, ce qui va conduire à un entrainement plus rapide. De plus, certains chercheurs affirment que cette normalisation permet de lisser la fonction d'apprentissage, ce qui permettrait en théorie d'obtenir moins de minimums locaux dans lesquels on resterai bloqué car trop profond.\n",
    "\n",
    "[Ici](https:\/\/link.springer.com\/article\/10.1007\/s10618-016-0483-9) les auteurs nous font également part de l'amélioration obtenue en utilisant la batch normalization pour des problèmes de classification. Néanmoins aujourd'hui de nouveaux réseaux de neurones appelé [Normalizer Free Nets](https:\/\/arxiv.org\/pdf\/2102.06171v1.pdf) ou \"NF-Nets\" sont parfois préferés à la batch normalization. L'implémentation n'étant pas disponnible en Keras, je me suis restreint à la batch normalization. \n",
    "\n",
    "Voici la définition de ce premier modèle :"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"hWEcYCtkH1vXFZ2NDtNiVU",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def create_first_version(input_shape,num_classes):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=32, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=32, kernel_size=3, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    pool = keras.layers.GlobalAveragePooling1D()(conv2)\n",
    "\n",
    "    classifier = keras.layers.Dense(num_classes, activation=\"softmax\")(pool)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=classifier)\n",
    "\n",
    "first_model = create_first_version(x_train.shape[1:],y_train.shape[1])\n",
    "first_model.summary()"
   ],
   "execution_count":109,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 30, 5)]           0         \n",
      "                                                                 \n",
      " conv1d_16 (Conv1D)          (None, 30, 32)            512       \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 30, 32)           128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_16 (ReLU)             (None, 30, 32)            0         \n",
      "                                                                 \n",
      " conv1d_17 (Conv1D)          (None, 30, 32)            3104      \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 30, 32)           128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_17 (ReLU)             (None, 30, 32)            0         \n",
      "                                                                 \n",
      " global_average_pooling1d_8   (None, 32)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,971\n",
      "Trainable params: 3,843\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"2iwSCFfDcHAT0l5i24MKFp",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "On obtient un nombre total de paramètres de ± 3900 ce qui est un nombre assez peu élevé. Cela s'explique par le fait que j'utilise seulement 32 filtres, contrairement au modèle initial qui en utilise 64. Encore, une fois j'ai diminué ce nombre car 64 me semblait trop élevé pour ce problème. (En revanche, le nombre de filtre augmentera dans le second modèle)\n",
    "\n",
    "### Essayons un entrainement avec ce modèle\n",
    "\n",
    "Pour l'entrainement j'ai fait le choix d'ajouter plusieurs [\"callbacks\"](https:\/\/keras.io\/api\/callbacks\/). Parmi tous les callbacks disponibles, je voulais initialement uniquement utiliser l'early stopping pour éviter le sur-apprentissage mais après avoir parcouru plusieurs exemples de modèles sur le site [Keras](https:\/\/keras.io), j'ai remarqué que beaucoup de modèle convolutionnels utilisaient le callback `ReduceLROnPlateau()`. Ce callback permet de réduire le learning rate lorsqu'une métrique (que l'on donne en paramètre) n'évolue plus, j'ai fait le choix d'ajouter ce callback car il me semblait pertinent étant donné que j'utilise la batch norm (donc potentiellement des learning rate plus élevé)."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"sPPBem1ZNmlLh8IyP2hIsZ",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#pour les paramètres j'ai choisit de stopper après 20 epochs lorsque le modele ne progresse plus sur le jeu de validation\n",
    "#car c'est à ce moment qu'on est potentiellement en train de sur-apprendre\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, verbose=1),\n",
    "             keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001),\n",
    "            keras.callbacks.ModelCheckpoint(\"ckpt.h5\", save_best_only=True, monitor=\"val_loss\")]"
   ],
   "execution_count":110,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"Hos8caEAYLtiGCSqBpjpiT",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "#j'utilise la categorial crossentropy car c'est la fonction de perte qui m'a semblé la plus pertinente lorsque l'on a un encodage\n",
    "#one-hot et la metrique \"categorical_accuracy\" car c'est également celle qui m'a semblé être pertinente pour l'encodage one-hot\n",
    "first_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])"
   ],
   "execution_count":111,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"lPvKfrXvsNmcjxe2vnGbib",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Entrainement du modèle"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"y7VpJyzd8pokh0uYalMRqn",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "#pour ce qui est des epoch j'ai choisit 200 car le reseau s'arrete généralement aux alentours de 50~70 epochs voire moins selon \n",
    "#l'architecture utilisée, on laisse donc l'early stopping arrêter le modèle seule.\n",
    "#j'ai également choisi de prendre 20% du jeu d'entrainement pour la validation car c'est ce sur ce jeu que vont s'appliquer\n",
    "#les callbacks\n",
    "history = first_model.fit(x_train, y_train, batch_size=64, epochs=200, callbacks=callbacks, validation_split=0.2, verbose=0)"
   ],
   "execution_count":112,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Epoch 00094: early stopping\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"nl6X6jORacaIUZifDfeuNj",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "L'entrainement dure environ 1min avec cette architecture. Voyons les résultats sur le jeu de test avant de passer à la présentation de l'architecture que j'ai utilisé finalement."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"V0N6VxkoVzXd2wzSAfFZsj",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "first_model = keras.models.load_model(\"ckpt.h5\")\n",
    "first_model.evaluate(x_test,y_test)"
   ],
   "execution_count":113,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "\r  1\/211 [..............................] - ETA: 36s - loss: 0.0627 - categorical_accuracy: 0.9375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 31\/211 [===>..........................] - ETA: 0s - loss: 0.0668 - categorical_accuracy: 0.9677 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 62\/211 [=======>......................] - ETA: 0s - loss: 0.0666 - categorical_accuracy: 0.9698\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 93\/211 [============>.................] - ETA: 0s - loss: 0.0660 - categorical_accuracy: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r124\/211 [================>.............] - ETA: 0s - loss: 0.0643 - categorical_accuracy: 0.9708\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r155\/211 [=====================>........] - ETA: 0s - loss: 0.0653 - categorical_accuracy: 0.9712\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r186\/211 [=========================>....] - ETA: 0s - loss: 0.0645 - categorical_accuracy: 0.9716\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r211\/211 [==============================] - 1s 2ms\/step - loss: 0.0658 - categorical_accuracy: 0.9709\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/plain":[
       "[0.06584995239973068, 0.9709328413009644]"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"t2quvmgxjkU1C3VVw0HNPg",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "On obtient une categorical accuracy entre 96 et 99% selon les runs qui est une très bonne accuracy pour le temps d'entrainements et le nombre de paramètres. J'ai néanmoins éssayer d'améliorer le modèle en essayant de le rendre encore plus rapide car la perte d'un ou deux pourcent de précision n'entrainerait pas de baisse significative des performances. Pour alléger le modèle j'ai d'abord essayé d'enlever une puis les deux batch norm pour voir les resultats obtenus. Les résultats à l'entrainements étaient bons mais je trouvais que le reseau généralisait moins bien pour un entrainement plus long, tout cela malgré l'early stopping. On parle d'une perte de 5% à 6% de précision sur les données de tests. J'ai donc par la suite décidé de garder ces batch norm mais de rajouter une couche de pooling apres la première convolution. Après avoir essayé les différentes couches de pooling 1D (max,avg, global max et global avg) j'ai décidé de garder l'average pooling qui donnait de meilleurs résultats sur les données d'entrainements et les données de tests.\n",
    "\n",
    "Par la suite j'ai essayé d'ajouter une couche de pooling en fin de deuxieme convolution mais cette couche ne changeait pas significativement les résultats, ils étaient presque similaires à ceux obtenus avec une seule couche de pooling.\n",
    "\n",
    "Le modèle final est donc très similaire au premier à la seule différence que l'on a rajouté une couche de pooling et 16 filtres :   "
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"DwJMrBqN6uwQabJLX6osBJ",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def create_second_version(input_shape,num_classes):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=48, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.AvgPool1D(pool_size=2,strides=1,padding=\"same\")(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=48, kernel_size=3, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    pool = keras.layers.GlobalAveragePooling1D()(conv2)\n",
    "\n",
    "    classifier = keras.layers.Dense(num_classes, activation=\"softmax\")(pool)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=classifier)\n",
    "\n",
    "second_model = create_second_version(input_shape=x_train.shape[1:],num_classes=y_train.shape[1])\n",
    "second_model.summary()"
   ],
   "execution_count":114,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 30, 5)]           0         \n",
      "                                                                 \n",
      " conv1d_18 (Conv1D)          (None, 30, 48)            768       \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 30, 48)           192       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " average_pooling1d_6 (Averag  (None, 30, 48)           0         \n",
      " ePooling1D)                                                     \n",
      "                                                                 \n",
      " re_lu_18 (ReLU)             (None, 30, 48)            0         \n",
      "                                                                 \n",
      " conv1d_19 (Conv1D)          (None, 30, 48)            6960      \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 30, 48)           192       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_19 (ReLU)             (None, 30, 48)            0         \n",
      "                                                                 \n",
      " global_average_pooling1d_9   (None, 48)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 147       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,259\n",
      "Trainable params: 8,067\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"L8dnOkFHRWOsPUquqr4HU6",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Entrainement avec le second modèle\n",
    "Pour l'entrainement on garde les mêmes hyper-paramètres. Le choix des hyper-paramètres sera détaillé dans la prochaine section"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"HI5WhNvfhoXZGLYHOMRvRQ",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20, verbose=1),\n",
    "             keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001),\n",
    "            keras.callbacks.ModelCheckpoint(\"ckpt2.h5\", save_best_only=True, monitor=\"val_loss\")]"
   ],
   "execution_count":115,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"VDZgTp53cQuuLcChTat92D",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "second_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])"
   ],
   "execution_count":119,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"dsPSPlO5DP485I5JwRqmaY",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "history2 = second_model.fit(x_train, y_train, batch_size=64, epochs=200, callbacks=callbacks, validation_split=0.2, verbose=0)"
   ],
   "execution_count":120,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Epoch 00058: early stopping\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"dhkKCuhNi3g3lH6Dpf7jqa",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "second_model = keras.models.load_model(\"ckpt2.h5\")\n",
    "second_model.evaluate(x_test,y_test)"
   ],
   "execution_count":121,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "\r  1\/211 [..............................] - ETA: 39s - loss: 0.0681 - categorical_accuracy: 0.9375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 28\/211 [==>...........................] - ETA: 0s - loss: 0.0645 - categorical_accuracy: 0.9721 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 54\/211 [======>.......................] - ETA: 0s - loss: 0.0655 - categorical_accuracy: 0.9722\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 80\/211 [==========>...................] - ETA: 0s - loss: 0.0678 - categorical_accuracy: 0.9703\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r105\/211 [=============>................] - ETA: 0s - loss: 0.0650 - categorical_accuracy: 0.9732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r132\/211 [=================>............] - ETA: 0s - loss: 0.0658 - categorical_accuracy: 0.9718\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r157\/211 [=====================>........] - ETA: 0s - loss: 0.0661 - categorical_accuracy: 0.9717\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r183\/211 [=========================>....] - ETA: 0s - loss: 0.0658 - categorical_accuracy: 0.9720\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r208\/211 [============================>.] - ETA: 0s - loss: 0.0670 - categorical_accuracy: 0.9716\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r211\/211 [==============================] - 1s 2ms\/step - loss: 0.0672 - categorical_accuracy: 0.9715\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/plain":[
       "[0.06715212017297745, 0.971526026725769]"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"3SLYt8TrwyCgJq5FFk7gok",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "On obtient encore de très bons résultats (souvent meilleurs que le premier modèle) pour un entrainement qui est en général également plus court."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"OkQ0547E232mZJB2wZ1g4C",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Choix des hyper-paramètres \n",
    "Pour les hyper-paramètres, j'ai surtout modifié sur les suivants :\n",
    "- le nombre de filtres\n",
    "- la taille du batch\n",
    "- la taille de la fenêtre de convolution\n",
    "\n",
    "#### Taille du batch\n",
    "Pour la taille du batch, j'ai essayé plusieurs valeurs entre 64 et 32. Pour un batch size de 32 l'entrainement est fatalement plus long cependant la précision n'est pas impactée, on peut donc penser que la réduction de la taille du batch n'est pas intéressante pour ce problème. On conserve donc une taille de batch de 64.\n",
    "\n",
    "#### Taille de la fenetre de convolution (kernel size)\n",
    "J'avais initialement mis la taille de la fenêtre de convolution à 5 me disant que c'était le nombre d'attribut d'un pas de temps cela me semblait donc cohérent mais je me suis rapidement rendu compte que le modèle généralisait très mal avec une si grande fenêtre de convolution. J'ai donc diminué la taille à 4 puis 3 puis 2 mais les meilleurs résultats que j'ai obtenus étaient avec une fenêtre de taille 3."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"Ix9ZDOscW0HxhwBbLHYcv1",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Validation des 2 modèles et comparaisons des performances\n",
    "Pour valider les modèles, étant donné que nous disposons déja de l'accuracy, nous allons nous servir du F1-score qui est plus représentatif. En effet le F1-score est utile lorsque les classes sont réparties inéquitablement (ce qui est le cas ici pour la classe 0 qui est sous représentée). Le F1-score va nous permettre d'avoir une interprétation de l'accuracy et du recall.\n",
    "\n",
    "Nous allons de plus calculer la matrice de confusion pour voir où ont lieux les erreurs.\n",
    "\n",
    "Pour commencer, calculons les prédictions pour le jeu de test avec la fonction `predict()`"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"O2AJZEK1KvmGogVFLvMTAu",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "prediction_model1 = first_model.predict(x_test)\n",
    "prediction_model2 = second_model.predict(x_test)"
   ],
   "execution_count":122,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"B175lTiavrzUV7wJUC0GiX",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "f1score_model1 = f1_score(y_test.argmax(axis=1),prediction_model1.argmax(axis=1),average=None)\n",
    "f1score_model2 = f1_score(y_test.argmax(axis=1),prediction_model2.argmax(axis=1),average=None)"
   ],
   "execution_count":123,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"7meKzFhMuRuZjjfX2akKrz",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "print(f1score_model1)\n",
    "print(f1score_model2)"
   ],
   "execution_count":124,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "[0.84385965 0.95559583 0.99773071]\n",
      "[0.84017467 0.95656109 0.99886378]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"QfMtArGupMTUoAkJXuN81e",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "On voit vraisemblablement que les deux modèles sont excellents sur les 2 dernières classes mais qu'ils ont plus de mal sur la première. Cela peut s'expliquer par le fait que, comme dit précedemment, la classe 0 est sous-représentée dans le jeu de données, le modèle manque donc peut être d'exemple pour cette classe.\n",
    "\n",
    "Malgré ça, les prédictions restent très bonnes, on voit qu'en moyenne pour une bonne prédiction le modèle fait très peu d'erreurs.  "
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"GZGaSRTfEkoWiYp682KYnx",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Matrice de confusion\n",
    "La matrice de confusion va nous permettre de voir ou le modèle commet des erreurs."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"MbiF8AD0VP9jukmqwEk3Po",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "matrix1 = confusion_matrix(y_test.argmax(axis=1),prediction_model1.argmax(axis=1))\n",
    "matrix2 = confusion_matrix(y_test.argmax(axis=1),prediction_model2.argmax(axis=1))"
   ],
   "execution_count":125,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"rTlPQVLuUZtsDkvFYQpRM0",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "f, axes = plt.subplots(1, 2, figsize=(20, 5), sharey='row')\n",
    "\n",
    "disp1 = ConfusionMatrixDisplay(matrix1)\n",
    "disp1.plot(ax=axes[0])\n",
    "disp1.ax_.set_title(\"Premier modèle\")\n",
    "disp1.im_.colorbar.remove()\n",
    "\n",
    "disp2 = ConfusionMatrixDisplay(matrix2)\n",
    "disp2.plot(ax=axes[1])\n",
    "disp2.ax_.set_title(\"Second modèle\")\n",
    "disp2.im_.colorbar.remove()\n",
    "\n",
    "plt.subplots_adjust()\n",
    "\n",
    "\n",
    "f.colorbar(disp2.im_, ax=axes)\n",
    "plt.show()"
   ],
   "execution_count":126,
   "outputs":[
    {
     "data":{
      "image\/png":[
       "iVBORw0KGgoAAAANSUhEUgAAA4YAAAFNCAYAAABcw3FTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy\/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA3wklEQVR4nO3deZhcZZX48e\/pzkoSEkIihCRAhIBGlGUigcEFWWRRn6AzjjiOoqKIgis6P1xmQBTEccBlRBRGRtxAHHWMioZFEVBZAgZkMRAhkA1CErIRsnT3+f1Rt6EI6aU6VV2dqu\/nee6Tqvdu515CTp9+3\/veyEwkSZIkSc2rpd4BSJIkSZLqy8JQkiRJkpqchaEkSZIkNTkLQ0mSJElqchaGkiRJktTkLAwlSZIkqclZGKpfRMTuEbEuIlrrHUtPIuLwiFhU9n1YRFwbEf+wlW3Pjojv92+EkiT1r4h4Z0TcXKdzL4iIo8q+nxIRP42I2GK7PSMiI2JQ\/0cpbf8sDJtM8Y\/r00WR9nhEfCciRtb6vJn5aGaOzMz2Wp+r2jJzA3ACcEpEHFDfaCRJjSgiXhERf4yI1RGxMiL+EBEvr3dcA1FmXgL8Hvh8vWORGom\/UWlOb8jM6yJiIjAb+AxwZvkGETEoM9vqEt0WBkIsmfkUcEw9Y5AkNaaI2BH4JfB+4CpgCPBKYGM94xrIMvOr9Y5BajT2GDaxzFwM\/BrYD6AYfnFaRDwIPFi0vT4i5kbEquI3mS\/r3L\/offxERNwdEU9FxLcjYpeI+HVErI2I6yJip2Lb5wzviIjRxfZLI2JxRHy+c5hpMVzlDxHx5YhYAZy9ZezFEM4fR8T3i3P9JSL2iYhPRsSyiFgYEa8t2363iJhV\/BZ2fkS8t2zd8KLn9MmIuA94+Rbn2i0ifhIRTxTX\/NGu7mlEHFLcp1URcVdEHF7pfxdJUtPZByAzr8jM9sx8OjOvycy7OzeIiHdHxP1FrpodEXuUrXtJ8cjDymI00KeK9qER8ZWIWFIsX4mIocW6wyNiUUScUeTNpRHxrrJj7lzkzTURcRuwV1fBl+X4dxX598mIODUiXl78jLAqIr5etn1LRHwmIh4pzv3diBhdtv7txboVEfHpLc7VEhFnRsTfivU\/johxXcTV5c8akp7PwrCJRcRk4Hjgz2XNJwAzgGkRcSBwGfA+YGfgW8CszqRS+AfgaEpJ7Q2UCs1PAeMp\/f36UBen\/w7QBuwNHAi8FnhP2foZwEPALsC5XRzjDcD3gJ2Ka5hdnHMicE4Rb6crgUXAbsA\/AudFxBHFurMoJby9KPUKntS5U0S0AL8A7gEmAUcBH46I120ZTJR6YH9FaWjLWODjwE8iYnwX8UuSBPAA0B4Rl0fEcZ2\/VO0UETMp5dY3UcqvNwFXFOtGAdcBv6GU4\/YGri92\/TRwCHAAsD9wMKVRQp12BUZTypsnAxeVnfsiYAMwAXh3sfRkBjAVeAvwleL8RwEvAf4pIl5dbPfOYnkN8EJgJPD14nqmARcDby+uZ2dK+bfTB4v78Joi7tXF9lvzHbr\/WUNSucx0aaIFWACsA1YBjwDfAIYX6xI4omzbi4HPbbH\/PODVZcd6W9m6nwAXl33\/IPB\/xec9i+MPolTsbew8b7H+rcDvis\/vBB7t4TrOBq4t+\/6G4rpai++jivONASYD7cCosu2\/AHyn+PwQcGzZulOARcXnGZQKyihb\/6myfc8Gvl98\/n\/A97aIczZwUr3\/u7u4uLi4DOwFeDGlQmYRpWJmFrBLse7XwMll27YA64E9ivz55y6O+Tfg+LLvxwALis+HA08Dg8rWL6NUSLYCm4EXla07D7i5i\/N05viJZW0rgLeUff8J8JHi8\/XAB8rW7VucbxDw78CVZetGAJuAo4rv9wNHl63frWzfXv+s4eLi8vzFZwyb0wmZeV0X6xaWfd4DOCkiPljWNoTSP8KdHi\/7\/PRWvm9tYps9gMHA0nh2QrGWLc69cMudtmLLcy3PZye3ebr4c2QR78rMXFu2\/SPA9OLzbluc75EtYh0L3F8W61Ce28tavu2bI+INZW2Dgd\/14lokSU0sM++n9ItRIuJFwPcp9bq9lVJ++WpEXFC2S1DqMZtMqQDcmt14bk57hOfm8BX53Gf411PKm+MpFVdd5cau9PZngq3F1VnMPScnZ+ZTxWMlnfYAvhkRm8vaVhf7ssV2Pf2sIamMhaG2lGWfFwLnZmZXQzn7aiGl3+KNy64nlcku2vtiCTA2IkaVFYe7A4uLz0spJdZ7y9aVx7o0M1\/Ui\/MspNRj+N4et5QkqQuZ+deI+A6lRzng2Xz8gy23LZ41PLGLQy2hVCCV57clvQjhCUq9lpOBv5btWy2dcXXavTjf45Ry8os7V0TEDpSGk3ZaCLwnM2\/a8qARsecW2\/X0s4akMj5jqO5cCpwaETOiZEREvK54nqHPMnMpcA1wQUTsWDxIvlfZswdVlZkLgT8CX4jSOwlfRulZis73D14FfDIidoqISZSGwHa6DVgdEZ8qJqlpjYj9YutTiH8feENEHFNsN6x4uH\/SVraVJAko9RAWk8BMKr5PptRTeEuxyTcp5amXFOtHR8Sbi3W\/BCZExEeKyWZGRcSMYt0VwGciYnwxQcu\/82zu61Ix+uanwNkRsUPx3N9JPexWiSuAj0bElCi9Mus84EdFAfe\/wOuj9PqOIZTmDCj\/efWblOYJmAJQXNvMrVxDv\/6sITUCC0N1KTPnAO+l9ED4k8B8imEuVfAOSsNS7yuO\/b+UHnCvlbdSevZgCfAz4Kyy4bSfpTSM5WFKSeR7nTsVyfH1wEuL9cuB\/6b0sP5zFAVo5wQBT1D6beUn8P8zSVL31lJ6pv3WiHiKUkF4D3AGQGb+DPgicGVErCnWHVesW0tpErg3AI9RmlX8NcVxPw\/MAe4G\/gLcSe\/f\/Xc6paGfj1F69vF\/tuUCt3AZpVx7I6XcuoHil7KZeS9wGvBDSr2HT1J67rLTVynl8d9ExFpK92oGW9ffP2tI27XIrOaIPUmSJEnS9saeDEmSJElqchaGkiRJktTkLAwlSZIkqclZGEqSJElSk7MwlCRJkqQmN6BecD9k0A45fPDz3gKgGsgNG+sdglQ1G3iKTbkx6h2H1IjMzf3H3KxG0ui5+ZjXjMgVK9sr3u+OuzfOzsxjaxDSNhtQheHwwaM5dK931zuMptB+3wP1DkGqmlvz+nqHIDUsc3P\/MTerkTR6bl6+sp1bZ0+qeL\/BE\/42rgbhVMWAKgwlSZIkaeBL2rOj3kFUlYWhJEmSJFUggQ6y3mFUlYWhJEmSJFWoA3sMJUmSJKlpJUl72mMoSZIkSU3NoaSSJEmS1MQSaLcwlCRJkqTm1mg9hi31DkCSJEmSVF\/2GEqSJElSBRKcfEaSJEmSml1jvazCwlCSJEmSKpKkk89IkiRJUlNLaG+sutDCUJIkSZIqkTiUVJIkSZKaXNBO1DuIqrIwlCRJkqQKJNDhUFJJkiRJam72GEqSJElSE0ssDCVJkiSp6XWkhaEkSZIkNS17DCVJkiSpySVBOy31DqOqLAwlSZIkqUIOJZUkSZKkJuZQUkmSJElqekF7OpRUkiRJkppWAh0N9oxhY12NJEmSJKli9hhKkiRJUoUa7RlDewwlSZIkqQKZpWcMK126ExHDIuK2iLgrIu6NiM8W7d+JiIcjYm6xHFC0R0R8LSLmR8TdEXFQ2bFOiogHi+Wk3lyTPYaSJEmSVKGO6vcYbgSOyMx1ETEYuDkifl2s+0Rm\/u8W2x8HTC2WGcDFwIyIGAucBUyn9DjkHRExKzOf7O7k9hhKkiRJUgVKr6toqXjp9pgl64qvg4slu9llJvDdYr9bgDERMQE4Brg2M1cWxeC1wLE9XZOFoSRJkiRVpM9DScdFxJyy5ZTnHDWiNSLmAssoFXe3FqvOLYaLfjkihhZtE4GFZbsvKtq6au+WQ0klSZIkqQLb8LqK5Zk5vcvjZrYDB0TEGOBnEbEf8EngMWAIcAnw\/4Bz+nLy7thjKEmSJEkVas+oeOmtzFwF\/A44NjOXFsNFNwL\/AxxcbLYYmFy226Sirav2blkYSpIkSVIFkqj6M4YRMb7oKSQihgNHA38tnhskIgI4Abin2GUW8I5idtJDgNWZuRSYDbw2InaKiJ2A1xZt3XIoqSRJkiRVqKOH10\/0wQTg8ohopdSBd1Vm\/jIifhsR44EA5gKnFttfDRwPzAfWA+8CyMyVEfE54PZiu3Myc2VPJ7cwlCRJkqQKdM5KWtVjZt4NHLiV9iO62D6B07pYdxlwWSXntzCUJEmSpAoklT0zuD2wMJQkSZKkCvVxVtIBy8JQkiRJkiqQSed7CRuGhaEkSZIkVSTowKGkvRYRxwJfBVqB\/87M82t5vlppaUm++o3rWLF8OGd\/5hXsf+DjnHzK3UQkGzYM4sL\/OJilS0ay30uf4JQPzGXKC1dz\/ucP4Q83Tap36A1h+uFrOPVzS2htSX59xViu+vou9Q6pYXmvJW0vzM31Zb7oP97rgSlpvB7Dml1NMc3qRcBxwDTgrRExrVbnq6WZb3yQhY+Oeub76R++ky99YQYfPPW13PDb3TnxbfcBsGzZDlz4Hy\/nht\/uXq9QG05LS3LaeYv5zNum8N7D9+U1M1ex+9QN9Q6rIXmvJW1PzM31Y77oP97rga3a7zGst1pGdzAwPzMfysxNwJXAzBqeryZ2Hreel89YyuyrX\/hMWybssMNmAEaM2MzKFcMBWPb4CBY8PIaOjrqE2pD2PXA9SxYM4bFHh9K2uYUbfj6GQ49ZXe+wGpL3WtL2wtxcX+aL\/uO9Vn+q5VDSicDCsu+LgBk1PF9NvO8Dc7ns0pcxvEg2AF+9YDqfPe9mNm1sZf36QXz0g0fWMcLGtvOum3liyZBnvi9fOpgXHbS+jhE1Lu+1pO2Fubm+zBf9x3s9cCVBR4O9rqLu\/ZkRcUpEzImIOZvaB9Zf9INnLGHVqmHMf3Cn57Sf8A8PctanXsE73vp6rp09hVNOvatOEUqSVH3mZknqWaMNJa1lj+FiYHLZ90lF23Nk5iXAJQCjh0\/IGsZTsWn7reCQQ5fw8oOXMnhIOzvs0MbZ597E5MlrmffXnQG48YbJfO4LN9Y50sa14rHBjN9t0zPfx03YzPKlg+sYUePyXkvqZG5Wd8wX\/cd7PXAl0OHkM712OzA1IqZExBDgRGBWDc9Xdd\/59kt5x1tfz7v+5XV88dxDuHvuCzjn3w5jhxGbmThxLQAHHvQ4Cx\/dsc6RNq55c3dg4pRN7DJ5I4MGd3D4zFXccs3oeofVkLzXkrYH5ub6M1\/0H+\/1QBa092EZyGrWY5iZbRFxOjCb0usqLsvMe2t1vv7S0dHC1y6czqfP\/iMdHcG6dUP4yn9OB2Dqviv5t7P\/yMiRm5hx6FL+5aR7ef97jqlzxNu3jvbgok9P5LwfPkRLK1xz5VgeeWBYvcNqSN5rSdsrc3P\/Ml\/0H+\/1wNWIPYaROXBGiIwePiEP3evd9Q6jKbTf90C9Q5Cq5ta8njW5cmD\/Gk7aTpmb+4+5WY2k0XPzpP1G52lXHVbxfp96ya\/vyMzpNQhpm9X0BfeSJEmS1Ggyo+F6DC0MJUmSJKlC7RaGkiRJktS8EugY4JPJVMrCUJIkSZIqEvYYSpIkSVIzK81Kao+hJEmSJDW19pq+Er7\/WRhKkiRJUgWSsMdQkiRJkppdhz2GkiRJktS8MqHdHkNJkiRJam6NNpS0sfo\/JUmSJEkVs8dQkiRJkipQmnymsfrYGutqJEmSJKkftBMVL92JiGERcVtE3BUR90bEZ4v2KRFxa0TMj4gfRcSQon1o8X1+sX7PsmN9smifFxHH9OZ6LAwlSZIkqQKdL7ivdOnBRuCIzNwfOAA4NiIOAb4IfDkz9waeBE4utj8ZeLJo\/3KxHRExDTgReAlwLPCNiGjt6eQWhpIkSZJUkdJQ0kqX7mTJuuLr4GJJ4Ajgf4v2y4ETis8zi+8U64+MiCjar8zMjZn5MDAfOLinK7IwlCRJkqQKdRAVLz2JiNaImAssA64F\/gasysy2YpNFwMTi80RgIUCxfjWwc3n7VvbpkpPPSJIkSVIFtuE9huMiYk7Z90sy85Jnj5vtwAERMQb4GfCibQq0AhaGkiRJklShPs5Kujwzp\/e0UWauiojfAYcCYyJiUNErOAlYXGy2GJgMLIqIQcBoYEVZe6fyfbrkUFJJkiRJqkDpdRXVnXwmIsYXPYVExHDgaOB+4HfAPxabnQT8vPg8q\/hOsf63mZlF+4nFrKVTgKnAbT1dkz2GkiRJklSh3jwzWKEJwOXFDKItwFWZ+cuIuA+4MiI+D\/wZ+Hax\/beB70XEfGAlpZlIycx7I+Iq4D6gDTitGKLaLQtDSZIkSapA5+sqqnrMzLuBA7fS\/hBbmVU0MzcAb+7iWOcC51ZyfgtDSZIkSapQH58xHLAsDCVJkiSpEr17Yf12xcJQkiRJkiqQ1OQZw7qyMJQkSZKkCtljKEmSJElNrBaTz9SbhaEkSZIkVajRCsPGmkpHkiRJklQxewwlSZIkqQKJs5JKkiRJUtNzVlJJkiRJambZeM8YWhhKkiRJUgWclVSSJEmSZGEoSZIkSc3MyWckSZIkSaSFoSRJkiQ1N2cllSRJkqQmls5KKkmSJElyKKkkSZIkNTUnn5EkSZKkpmePYS1tboMlj9c7iqbwwMUH1zuEprDPB26vdwjNIesdgNTAzM39xtzcP\/Z5\/231DkENwBfcS5IkSVKzy9IENI3EwlCSJEmSKuTrKiRJkiSpiSU+YyhJkiRJTa7xZiVtqXcAkiRJkqT6sjCUJEmSpAplVr50JyImR8TvIuK+iLg3Ij5ctJ8dEYsjYm6xHF+2zycjYn5EzIuIY8rajy3a5kfEmb25HoeSSpIkSVKFavCMYRtwRmbeGRGjgDsi4tpi3Zcz8z\/LN46IacCJwEuA3YDrImKfYvVFwNHAIuD2iJiVmfd1d3ILQ0mSJEmqQKkHsLqFYWYuBZYWn9dGxP3AxG52mQlcmZkbgYcjYj7Q+ULU+Zn5EEBEXFls221h6FBSSZIkSapQR0bFS29FxJ7AgcCtRdPpEXF3RFwWETsVbROBhWW7LSraumrvloWhJEmSJFWoj88YjouIOWXLKVseNyJGAj8BPpKZa4CLgb2AAyj1KF5Qi+txKKkkSZIkVaiPQ0mXZ+b0rlZGxGBKReEPMvOnpfPk42XrLwV+WXxdDEwu231S0UY37V2yx1CSJEmSKpAEmZUv3YmIAL4N3J+ZF5a1Tyjb7I3APcXnWcCJETE0IqYAU4HbgNuBqRExJSKGUJqgZlZP12SPoSRJkiRVqIe3T\/TFYcDbgb9ExNyi7VPAWyPigOKUC4D3AWTmvRFxFaVJZdqA0zKzHSAiTgdmA63AZZl5b08ntzCUJEmSpErUZlbSm4GtHfTqbvY5Fzh3K+1Xd7ff1lgYSpIkSVKlatBlWE8WhpIkSZJUoRq84L6uLAwlSZIkqUJpj6EkSZIkNa\/EHkNJkiRJam4JWBhKkiRJUnNzKKkkSZIkNbsGKwxb6h2AJEmSJKm+uuwxjIj\/ops6ODM\/VJOIJElS1ZjPJakWoqkmn5nTb1FIkqRaMZ9LUi002FDSLgvDzLy8\/HtE7JCZ62sfkiRJqhbzuSTVQDbe6yp6fMYwIg6NiPuAvxbf94+Ib9Q8MkmSVDXmc0mqsuzDMoD1ZvKZrwDHACsAMvMu4FU1jEmSJFXfVzCfS1IVRR+WgatXr6vIzIURz7mQ9tqEI0mSasV8LklVNMB7ACvVm8JwYUT8PZARMRj4MHB\/bcOSJElVZj6XpGpqsMKwN0NJTwVOAyYCS4ADiu+SJGn7YT6XpGpJIKPyZQDrsccwM5cDb+uHWCRJUo2YzyWpurLZegwj4oUR8YuIeCIilkXEzyPihf0RnCRJqg7zuSRVWRPOSvpD4CpgArAb8GPgiloGJUmSqs58LknV1GBDSXtTGO6Qmd\/LzLZi+T4wrNaBSZKkqjKfS1IVRVa+DGRdPmMYEWOLj7+OiDOBKyl1gL4FuLofYpMkSdvIfC5JNbAdDA2tVHeTz9xB6XI7+zzfV7YugU\/WKihJklQ15nNJqrqBPzS0Ul0Whpk5pT8DkSRJ1Wc+l6QaaaIew2dExH7ANMqeRcjM79YqKEmSVH3mc0mqomYrDCPiLOBwSonkauA44GbARCJJ0nbCfC5J6k5vZiX9R+BI4LHMfBewPzC6plFJkqRqM59LUjU12HsMezOU9OnM7IiItojYEVgGTK5xXAPGRz4\/j4NfvZJVKwfzgZnTATjzgvuZOGU9ACNHtbFu7SA++Ka\/o3VQBx8+5wH2nraOltbkt7N24apLd69n+APaoJUb2fXyh2hdsxkiWP2K8aw6YldG3rGSnX+1mCGPPc2j\/28aG\/cY+cw+O\/1mCaP\/+AREsOwtu7N+2hgAxvz2MUbf\/AQAqw8bz6ojd63HJW13PnbBo8w4ag2rlg\/ifUe+CIAXvmQ9Hzp\/EUOGdtDeFnz9U5OYN3dEnSOVVAUNk8+3lptf+KJ1nH7Wgwwe2kFHW3DR5\/bmgb\/syCFHLOftH3yEjoSOtuBb5+\/FfXdaD3el0tzcsm4zu106n2GPPMWaQ8ax7MQ9n3fM3b7xAIOXb+SRf39pP19NY3jje5\/guH9eQWbw8F+HccFHJ7N5Y2\/6dlRTSdUnn4mIyZRGcexSnOGSzPxqMbv0j4A9gQXAP2XmkxERwFeB44H1wDsz887iWCcBnykO\/fnMvLyn8\/emMJwTEWOASynNbLYO+FMvLuwy4PXAsszcrxfnGZCu+9ku\/OIHu3HG+fOeaTv\/jBc\/8\/k9\/\/o3nlpbuo2vPGY5g4ckHzhhOkOHtfPNX8zhhl+9gGVLfE3U1mRr8MQ\/7M7G3UcQG9rZ4wv3sP7Fo9m023CWnLI3u\/xwwXO2H7L0aXacs4JH\/u2ltK7ezKSv\/pUFnx3NkKVPM\/rmJ3j0zGlkawsT\/2seT710DJtf4H3vyTVXjWXW\/4zjE1999Jm293x6Kd+\/cFfm\/G5HXn7EGk7+9BL+9c1T6xilpCrpUz4fiLaWm999xkP88Bt7MOemsUx\/1UrefcbDnPnO\/Zl7y07c8tudgWDPfdbxyQvv532vf3n9gh\/gKs3NObiF5W+YxNAlTzN0yfrnHW\/kn1fSMdQipq923nUzJ5y8nPcevi+bNrTw6W8u4PCZq7j2qrE976yaq8F7CduAMzLzzogYBdwREdcC7wSuz8zzi9cOnQn8P0qPBEwtlhnAxcCMopA8C5hOqcC8IyJmZeaT3Z28x\/9TM\/MDmbkqM78JHA2cVAxB6cl3gGN7sd2Ads8dY1i7enAXa5NXHvMEv7\/6BaVvCcOGt9PSmgwZ2kHb5hbWP9Xaf8FuZ9pHD2Hj7qWeqBzWyqZdhzNo1SY2TRjO5l2HP2\/7EXc9yZrpO5ODW2gbN5TN44cybME6hjy2gQ1TRpBDWqE1eHqfUYyc2+3fexXuuXUka1c99+9oJowY1Q6U\/lz5eFd\/\/yVtT7Yhnw84W8vNmcEOI9oAGDGyjZXLhgCwYX0rnW\/qGDa8gxzgQ7nqrdLcnENb2bD3KHLw83tOYkM7O13\/GCuPn1jzuBtZ66Bk6LAOWlqTocM7WGFeHjiqPJQ0M5d29vhl5lrgfmAiMBPo7PG7HDih+DwT+G6W3AKMiYgJwDHAtZm5sigGr6UXdVl3L7g\/qLt1nUF3c2E3RsSePQWwPdvv71azasUQljxS+ofy5mvGccgRK\/jB729h6LB2LvniXqzrsqhUuUErNjJ04Xo27Dmyy20Gr9rE01OeXd+20xAGrdrMpt2GM27WQlrWbSaHtDDinlVs2MOhj331zbMmct4P\/8Z7\/20JEfDRmfYWStuzbc3n24tLzt+Lz136F07+xENEC3z8bQc8s+7QI5fzzo8+zJidN3PWqdvtIKZ+15vc3J1xv1jEk0ftSscQewz7asVjg\/nfi8fzvdvvZ+OG4M7fj+LO34+qd1jaNuMiYk7Z90sy85ItNyrqqAOBW4FdMnNpseoxSkNNoVQ0LizbbVHR1lV7t7obSnpBN+sSOKKngze6V7\/uCW4oegsB9n3pWjo64F8On8HIHdv40vfuYu6fxvDYouf\/hk3Pig3t7PatB3nizbvTMbzyHtZNE4az8rW7Melr8+gY2srGSSMgGuuFo\/3p9e9YzrfOnsjNV4\/hVW94ko9d8Chnnrh3vcOS1HdNkc+PP3EJl57\/Qv5w7XheeewTfPhzD\/Dpk18GwJ+uH8efrh\/Hfn+3ird\/aMEz7eratubmoQufYvATG3nizXswaMXGGkTYHEaObuPQY9Zw0owXs25NK5+5ZAFHvOlJfvvTneodmujzUNLlmTm92+NGjAR+AnwkM9dE2c+1mZkRNRjESvcvuH9NLU64pYg4BTgFYFjL9tPL09Ka\/P1Ry\/nQm5\/9Rezhr1vGHTeNpb2thdUrh3Dfn3dk6n7rLAy7097Bbpc8yJqDd2bdgd2Pl988ZgiDntz0zPdBT26ibUypR3bNYeNZc9h4AHb+v4W07TSkdjE3uKPfvJKL\/730S6UbfzGGj3xpYQ97SBrI+pLPt8fcfNTMx\/nWeXsBcNNvxvHhcx543jb33DGGXSc9wI5jNrNmlSN6ulRBbu7KsIfWMezRp5jy6bnQkQxa28akC+9n0cde3OO+etaBr1zHYwuHsHpl6Uf2P1w9mmnTn7IwHCiqPPkMQEQMplQU\/iAzf1o0Px4REzJzaTFUdFnRvpjnTiI2qWhbTOn1ROXtN\/R07rr37WfmJZk5PTOnD4ntp4A68NAnWfTwDqx4fOgzbcuWDmP\/Q1YBMHR4Oy\/afy0LH9p+rqnfZbLr9x5m067DWXXUhB43f+plY9hxzgpicweDlm9k8LKNzwxvaV2zGSjNpjZq7pOsffnONQ29ka14fDAvO3QdAAe8Yh1LHh7awx6SGs32mJtXLBvCS1++GoD9D1nF4uIxjwm7P03ngz17vXgtg4d0sGZVb+bea1IV5uaurH71Ljx0\/oE8fO4BLPz4NDa9YJhFYR8sWzyYFx\/0FEOHdwDJAa9Yx6PzzcsDQl+eL+yhn6+YZfTbwP2ZeWHZqlnAScXnk4Cfl7W\/I0oOAVYXQ05nA6+NiJ0iYifgtUVbt\/yXsQf\/+qX7ednBq9lxzGa++9tb+P7X9+Can07gVcc9we+vHv+cbX95xW589Nx5XDxrDhFw7c92YcEDfRuX3wyG\/W0dO966go0Th7P7ufcAsGLmJKKtg\/E\/eoTWdW1MvOgBNk7agcUfehGbdtuBtX+3M3uc8xdoCZaduAe0lH5TM+GSB2l9qg1ag8dP3IOOHfyr3RtnXrSAlx26jtFj2\/j+nHv53n\/uylc+MZn3n7OY1kHJpg0tfOVft8vZ7CU1sK3l5q+dtQ\/v++TfaG1NNm9q4b\/OKj0ffdjRyzly5uO0tQWbNrQUM4v7uEFXKs3NAFM+PZeWDe1EezLiridLOXvC9vELhYFu3p9HcNOvxnDR7Adobwvm3zOcX3\/fX34PGNUf0HkY8HbgLxExt2j7FHA+cFVEnAw8AvxTse5qSq+qmE\/pdRXvAsjMlRHxOeD2YrtzMnNlTyePrNH0XBFxBaUuzHHA48BZmfnt7vYZPWh8HrrjzJrEo+e6\/wv71juEprDPB27veSNts1s7rmNNrvQnPakGzM39x9zcP\/Z5\/231DqEp3JrXN3RuHjp5ck766Ecr3u+hM864o6dnDOulx26VokvzbcALM\/OciNgd2DUzu\/2\/KjPfWqUYJUnSNuprPpckdaHBXn\/Tm2cMvwEcCnQWemuBi2oWkSRJqgXzuSRVU5WfMay33jyINSMzD4qIPwNk5pMR4ZSPkiRtX8znklQlkX1+XcWA1ZvCcHNEtFLUuBExHuioaVSSJKnazOeSVE01eF1FPfVmKOnXgJ8BL4iIc4GbgfNqGpUkSao287kkVVOzDSXNzB9ExB3AkZTmdz4hM++veWSSJKlqzOeSVF1NN5S0mLVsPfCL8rbMfLSWgUmSpOoxn0uSutObZwx\/RanjM4BhwBRgHvCSGsYlSZKqy3wuSdXUbD2GmfnS8u8RcRDwgZpFJEmSqs58LklV1KSzkj5HZt4ZETNqEYwkSeof5nNJ2kbNVhhGxMfKvrYABwFLahaRJEmqOvO5JFVZsxWGwKiyz22UnlH4SW3CkSRJNWI+l6QqaqqhpMWLcEdl5sf7KR5JklRl5nNJUk+6LAwjYlBmtkXEYf0ZkCRJqh7zuSTVSBP1GN5G6fmDuRExC\/gx8FTnysz8aY1jkyRJ2858LknV1qSzkg4DVgBH8Oz7jxIwkUiStP0wn0tSNTVRYfiCYgaze3g2gXRqsNsgSVLDMp9LUi002L+g3RWGrcBInptAOjXYbZAkqWGZzyWpyoLmGkq6NDPP6bdIJElSLZjPJakWmqgw3NpvFiVJ0vbFfC5J1dZkk88c2W9RSJKkWjGfS1ItNEthmJkr+zMQSZJUfeZzSaqRZikMJUmSJElb12hDSVvqHYAkSZIkqb7sMZQkSZKkStljKEmSJElNLPu49CAiLouIZRFxT1nb2RGxOCLmFsvxZes+GRHzI2JeRBxT1n5s0TY\/Is7szSVZGEqSJElShSIrX3rhO8CxW2n\/cmYeUCxXA0TENOBE4CXFPt+IiNaIaAUuAo4DpgFvLbbtlkNJJUmSJKlSNRhKmpk3RsSevdx8JnBlZm4EHo6I+cDBxbr5mfkQQERcWWx7X3cHs8dQkiRJkipUox7DrpweEXcXQ013KtomAgvLtllUtHXV3i0LQ0mSJEmqVN+eMRwXEXPKllN6caaLgb2AA4ClwAVVvY6CQ0klSZIkqRK9nExmK5Zn5vSKTpX5eOfniLgU+GXxdTEwuWzTSUUb3bR3yR5DSZIkSapA9HHp07kiJpR9fSPQOWPpLODEiBgaEVOAqcBtwO3A1IiYEhFDKE1QM6un89hjKEmSJEmVqsHkMxFxBXA4pSGni4CzgMMj4oDijAuA9wFk5r0RcRWlSWXagNMys704zunAbKAVuCwz7+3p3BaGkiRJklShbZxMZqsy861baf52N9ufC5y7lfargasrObeFoSRJkiRVqgaFYT1ZGEqSJElSpSwMJUmSJKmJbft7CQccC0NJkiRJqpSFoSRJkiQ1N3sMJUmSJKnZWRhKkiRJUnOzx7CGsr2d9lWr6x1GU9jn\/bfVO4SmMHvJ3HqH0BQOPmZ9vUOQGpa5uf+Ym\/uHubl\/mJu3PwOqMJQkSZKkAS9xKKkkSZIkNT0LQ0mSJElqXoHPGEqSJEmSLAwlSZIkqblFNlZlaGEoSZIkSZVw8hlJkiRJks8YSpIkSVKzszCUJEmSpOZmj6EkSZIkNTsLQ0mSJElqYmmPoSRJkiTJwlCSJEmSmldgj6EkSZIkyRfcS5IkSVJzs8dQkiRJkppZ0nDPGLbUOwBJkiRJUn1ZGEqSJElShaKj8qXHY0ZcFhHLIuKesraxEXFtRDxY\/LlT0R4R8bWImB8Rd0fEQWX7nFRs\/2BEnNSb67EwlCRJkqRKZR+Wnn0HOHaLtjOB6zNzKnB98R3gOGBqsZwCXAylQhI4C5gBHAyc1VlMdsfCUJIkSZIqFFn50pPMvBFYuUXzTODy4vPlwAll7d\/NkluAMRExATgGuDYzV2bmk8C1PL\/YfB4nn5EkSZKkSiT9+bqKXTJzafH5MWCX4vNEYGHZdouKtq7au2VhKEmSJEkV6uPrKsZFxJyy75dk5iW93TkzM6I2L8qwMJQkSZKkSvWtPFuemdMr3OfxiJiQmUuLoaLLivbFwOSy7SYVbYuBw7dov6Gnk\/iMoSRJkiRVIKjNM4ZdmAV0zix6EvDzsvZ3FLOTHgKsLoaczgZeGxE7FZPOvLZo65Y9hpIkSZJUicyaPGMYEVdQ6u0bFxGLKM0uej5wVUScDDwC\/FOx+dXA8cB8YD3wrlJouTIiPgfcXmx3TmZuOaHN81gYSpIkSVKFavGkX2a+tYtVR25l2wRO6+I4lwGXVXJuC0NJkiRJqlS\/TUraPywMJUmSJKlCtZkbtH4sDCVJkiSpEgl0NFZlaGEoSZIkSZVqrLrQwlCSJEmSKuVQUkmSJElqdjV4XUU9WRhKkiRJUoUarcewpd4BSJIkSZLqyx5DSZIkSapE4uQzkiRJktTMAgifMZQkSZKkJtdR7wCqy8JQkiRJkipkj6EkSZIkNTOfMZQkSZKkZpe+x1Al0w9fw6mfW0JrS\/LrK8Zy1dd3qXdIDct7vW02bQjOeNPebN7UQnsbvPJ1q3nHJx5j7s0jufSc3di8OZj6sqf52AWP0joI7vrjSM5+1xR2nbwJgMOOX8W\/fOxxFs4fynmn7vnMcR97dAhv\/8RjvOm9T9TpyiTpucwX\/eNjFz7KjKPWsmr5IN53xL71Dme7VK3cDLBudStf\/vhkFvx1GBGl\/z7Tpq+v5+U1jUZ7j2HNCsOImAx8F9iFUkfrJZn51Vqdrz+1tCSnnbeYT574QpYvHcx\/Xf0gt8wezaMPDqt3aA3He73tBg9N\/uPHf2P4iA7aNsPHTpjK3x2+hi99eHe+eNXfmLTXRi7\/j1259qqxHPvPKwHYb8Y6Pvfdh59znMl7b+Ti6+YB0N4ObzvoJRx23Kr+vhxJ2irzRf+55kdjmfU\/4\/jEVxfWO5TtVrVyM8DF\/z6R6Yev4d8uXcDmTcHGp31Neb9psB7DWv7NaQPOyMxpwCHAaRExrYbn6zf7HrieJQuG8NijQ2nb3MINPx\/DocesrndYDcl7ve0iYPiI0rRZbZuD9s1BaysMHpJM2msjAAe9ei03Xz2m18ece9MoJuyxkV0mba5FyJJUMfNF\/7nn1pGsfdJBZ9uiWrn5qTUt\/OWWEc8Uj4OHJCNHt9c0dhUSoqPyZSCrWWGYmUsz887i81rgfmBirc7Xn3bedTNPLBnyzPflSwczboI\/INeC97o62tvh\/Ufty1teth8Hvmot+x64nva24IG7hgNw8y\/H8MSSwc9sf\/8dIzj1qH359NteyIJ5z\/9t+w0\/H8PhJ6zqr\/AlqUfmC21vqpGbH3t0KKN3buOCj+7OB47ehy+fMZkN6+0x7DeZlS8DWL\/8zYmIPYEDgVv743ySnqu1FS6+bh4\/uOM+5s3dgUfmDeOTFy\/gm2dN5IPHT2X4yHZain8N9n7per53231887p5zHz3E3z23VOec6zNm4JbrhnNq96wqv8vRJKkBlGN3NzeDvP\/sgOvf8dyvnHtAwzboYMfff0FdbyqJpN9WAawmheGETES+Anwkcxcs5X1p0TEnIiYs5mNtQ6nKlY8Npjxu2165vu4CZtZvnRwN3uor7zX1TVydDv7\/\/06bv\/dKKZNX8+F\/zef\/7r6QV464ykm7rUBgBGjOp4Z3nLwkWtp3xysXtH6zDFu\/+0o9n7penYa31aXa5BUe+Zmqf9sS24eN2Ez4yds5kUHlSabecXrVzH\/L8Prdi3NJjIrXgaymhaGETGYUlH4g8z86da2ycxLMnN6Zk4fzNBahlM18+buwMQpm9hl8kYGDe7g8JmruOWa0fUOqyF5r7fdqhWtrFtdKuw2Ph3ceeMoJu+9kVXLS8+HbNoYXPWNF\/D6t68AYOWyQc+MdPjrn3egowN2HPvs8wo3\/N9ODiOVGpy5WaqtauXmsS9oY9xum1g4v\/T\/6dybRrH71O3jlzkNocGGktZyVtIAvg3cn5kX1uo89dDRHlz06Ymc98OHaGmFa64cyyMPOOtZLXivt93Kxwfznx\/enY6OoKMDXvWGVRxy9BouPWc3br1uR7IDXnfSCg54xToAbvrlGH753Z1pHQRDh3XwyYsXEFE61ob1Ldx50yg+\/B\/ORCdpYDFf9J8zv\/EILzt0HaPHtvH9OffxvQt2YfYVO9c7rO1KNXPzaZ9fzBdP34O2zcGuu2\/ijC8\/WscrayIJDPDJZCoVWaPKNSJeAdwE\/IVnb9unMvPqrvbZMcbmjDiyJvFI9TB7ydx6h9AUDj5mIXPu2hD1jkNqROZmNRpzc\/9o9Nw8esRueci091W83zVzzr4jM6fXIKRtVrMew8y8GWjYvwySJEmSmtgAHxpaKeezlSRJkqQm59tJJUmSJKlS9hhKkiRJUhPrnHym0qUXImJBRPwlIuZGxJyibWxEXBsRDxZ\/7lS0R0R8LSLmR8TdEXFQXy\/JwlCSJEmSKlTj9xi+JjMPKJuo5kzg+sycClxffAc4DphaLKcAF\/f1eiwMJUmSJKlS\/fsew5nA5cXny4ETytq\/myW3AGMiYkJfTmBhKEmSJEkV6UNRWCoMx0XEnLLllK0fnGsi4o6y9btk5tLi82PALsXniUD5C6YXFW0Vc\/IZSZIkSapE0tcewOW9eI\/hKzJzcUS8ALg2Iv76nFNnZkRUfeYbewwlSZIkqVI1mnwmMxcXfy4DfgYcDDzeOUS0+HNZsfliYHLZ7pOKtopZGEqSJElShWox+UxEjIiIUZ2fgdcC9wCzgJOKzU4Cfl58ngW8o5id9BBgddmQ04o4lFSSJEmSKlWb9xjuAvwsIqBUq\/0wM38TEbcDV0XEycAjwD8V218NHA\/MB9YD7+rriS0MJUmSJKkSCXRUvzDMzIeA\/bfSvgI4civtCZxWjXNbGEqSJElSRbb59RMDjoWhJEmSJFXKwlCSJEmSmpyFoSRJkiQ1sRo9Y1hPFoaSJEmSVJGE7OWLCbcTFoaSJEmSVKkGG0rqC+4lSZIkqcnZYyhJkiRJlfAZQ0mSJElSow0ltTCUJEmSpEpZGEqSJElSM0sLQ0mSJElqagl0+LoKSZIkSWpu9hhKkiRJUpOzMJQkSZKkZpa+rkKSJEmSmlpCps8YSpIkSVJzs8dQkiRJkpqczxhKkiRJUhPL9HUVkiRJktT07DGUJEmSpOaW9hhKkiRJUjNLewwlSZIkqaklzkoqSZIkSU2vwd5j2FLvACRJkiRJ9WVhKEmSJEkVSCA7suKlJxFxbETMi4j5EXFm7a\/kWQ4llSRJkqRKZFZ9KGlEtAIXAUcDi4DbI2JWZt5X1RN1wcJQkiRJkirUmx7ACh0MzM\/MhwAi4kpgJmBhKEmSJEkDUvUnn5kILCz7vgiYUe2TdCVyAL1\/IyKeAB6pdxwVGgcsr3cQTcD73H+2x3u9R2aOr3cQUiMyN6sb3uf+sz3e64bOzRHxG0r\/XSo1DNhQ9v2SzLykOOY\/Asdm5nuK728HZmTm6dsab28MqB7D7fEvT0TMyczp9Y6j0Xmf+4\/3WlI5c7O64n3uP97rgSczj63BYRcDk8u+Tyra+oWzkkqSJElS\/d0OTI2IKRExBDgRmNVfJx9QPYaSJEmS1Iwysy0iTgdmA63AZZl5b3+d38Jw211S7wCahPe5\/3ivJW3v\/Hesf3if+4\/3uklk5tXA1fU494CafEaSJEmS1P98xlCSJEmSmpyFYR9FxLERMS8i5kfEmfWOp1FFxGURsSwi7ql3LI0uIiZHxO8i4r6IuDciPlzvmCSpEubm\/mFu7j\/mZvUnh5L2QUS0Ag8AR1N68eTtwFsz8766BtaAIuJVwDrgu5m5X73jaWQRMQGYkJl3RsQo4A7gBP9eS9oemJv7j7m5\/5ib1Z\/sMeybg4H5mflQZm4CrgRm1jmmhpSZNwIr6x1HM8jMpZl5Z\/F5LXA\/MLG+UUlSr5mb+4m5uf+Ym9WfLAz7ZiKwsOz7IvyfVA0kIvYEDgRurXMoktRb5mY1NHOzas3CUNJzRMRI4CfARzJzTb3jkSSp2Zmb1R8sDPtmMTC57Pukok3arkXEYEqJ5weZ+dN6xyNJFTA3qyGZm9VfLAz75nZgakRMiYghwInArDrHJG2TiAjg28D9mXlhveORpAqZm9VwzM3qTxaGfZCZbcDpwGxKDwFflZn31jeqxhQRVwB\/AvaNiEURcXK9Y2pghwFvB46IiLnFcny9g5Kk3jA39x9zc78yN6vf+LoKSZIkSWpy9hhKkiRJUpOzMJQkSZKkJmdhKEmSJElNzsJQkiRJkpqchaEkSZIkNTkLQ21VRLQXUyLfExE\/jogdtuFY34mIfyw+\/3dETOtm28Mj4u\/7cI4FETGut+1bbLOuwnOdHREfrzRGSZK2hbm52+3NzdI2sjBUV57OzAMycz9gE3Bq+cqIGNSXg2bmezLzvm42ORyoOPlIktQEzM2SasbCUL1xE7B38RvDmyJiFnBfRLRGxJci4vaIuDsi3gcQJV+PiHkRcR3wgs4DRcQNETG9+HxsRNwZEXdFxPURsSelJPfR4jeir4yI8RHxk+Ict0fEYcW+O0fENRFxb0T8NxA9XURE\/F9E3FHsc8oW675ctF8fEeOLtr0i4jfFPjdFxIuqcjclSdp25mZzs1RVffrNkppH8dvH44DfFE0HAftl5sPFP+CrM\/PlETEU+ENEXAMcCOwLTAN2Ae4DLtviuOOBS4FXFccam5krI+KbwLrM\/M9iux8CX87MmyNid2A28GLgLODmzDwnIl4HnNyLy3l3cY7hwO0R8ZPMXAGMAOZk5kcj4t+LY58OXAKcmpkPRsQM4BvAEX24jZIkVY252dws1YKFoboyPCLmFp9vAr5NaRjJbZn5cNH+WuBlUTyjAIwGpgKvAq7IzHZgSUT8divHPwS4sfNYmbmyiziOAqZFPPNLxx0jYmRxjjcV+\/4qIp7sxTV9KCLeWHyeXMS6AugAflS0fx\/4aXGOvwd+XHbuob04hyRJtWJuNjdLNWNhqK48nZkHlDcU\/wg\/Vd4EfDAzZ2+x3fFVjKMFOCQzN2wlll6LiMMpJbJDM3N9RNwADOti8yzOu2rLeyBJUh2Zm83NUs34jKG2xWzg\/RExGCAi9omIEcCNwFuK5xwmAK\/Zyr63AK+KiCnFvmOL9rXAqLLtrgE+2PklIg4oPt4I\/HPRdhywUw+xjgaeLBLPiyj9VrRTC9D5m9V\/pjQMZg3wcES8uThHRMT+PZxDkqR6MzdL6hMLQ22L\/6b0jMKdEXEP8C1KvdA\/Ax4s1n0X+NOWO2bmE8AplIaG3MWzw0V+Abyx8wF34EPA9Cg9QH8fz87A9llKyeteSsNWHu0h1t8AgyLifuB8Ssmv01PAwcU1HAGcU7S\/DTi5iO9eYGYv7okkSfVkbpbUJ5GZ9Y5BkiRJklRH9hhKkiRJUpOzMJQkSZKkJmdhKEmSJElNzsJQkiRJkpqchaEkSZIkNTkLQ0mSJElqchaGkiRJktTkLAwlSZIkqcn9f8PI5gly\/XVJAAAAAElFTkSuQmCC\n"
      ]
     },
     "metadata":{
      "image\/png":{
       
      }
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"gDb5q1Kbyqq45nn5lJsXPH",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "On remarque avec les matrices de confusion que le second modèle a de meilleures performances, notamment sur la prédiction de la première classe. Cette amélioration est certainement due aux 16 filtres supplémentaires dont dispose ce modèle ce qui accroit encore ses performances. Néanmoins, ces filtres supplémentaires augmentent relativement le nombre de paramètres (il est presque doublé pour le deuxième modèle mais ce nombre reste tout à fait raisonnable).\n",
    "\n",
    "On constate également que les erreurs ont lieux aux intersections entre 2 classes. Par exemple pour le second modèle, quand il prédit la classe 2, il fait uniquement des erreurs avec la classe 1 qui est la classe de transition survenant juste avant la classe 2 représentant le problème dans les tuyaux. On peut donc en déduire que l'aspect temporel est \"compris\" par le réseaux (tel évenement survient avant ou après tel autre). "
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"lLp1VMgAmWGRorGqOZADFW",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "widgets":{
   "application\/vnd.jupyter.widget-state+json":{
    "version_major":2,
    "version_minor":0,
    "state":{
     "8e9699b4b67145cba596d6cd2130376f":{
      "model_name":"LayoutModel",
      "model_module":"@jupyter-widgets\/base",
      "model_module_version":"1.2.0",
      "state":{
       "_model_module":"@jupyter-widgets\/base",
       "_model_module_version":"1.2.0",
       "_model_name":"LayoutModel",
       "_view_count":null,
       "_view_module":"@jupyter-widgets\/base",
       "_view_module_version":"1.2.0",
       "_view_name":"LayoutView",
       "align_content":null,
       "align_items":null,
       "align_self":null,
       "border":null,
       "bottom":null,
       "display":null,
       "flex":null,
       "flex_flow":null,
       "grid_area":null,
       "grid_auto_columns":null,
       "grid_auto_flow":null,
       "grid_auto_rows":null,
       "grid_column":null,
       "grid_gap":null,
       "grid_row":null,
       "grid_template_areas":null,
       "grid_template_columns":null,
       "grid_template_rows":null,
       "height":null,
       "justify_content":null,
       "justify_items":null,
       "left":null,
       "margin":null,
       "max_height":null,
       "max_width":null,
       "min_height":null,
       "min_width":null,
       "object_fit":null,
       "object_position":null,
       "order":null,
       "overflow":null,
       "overflow_x":null,
       "overflow_y":null,
       "padding":null,
       "right":null,
       "top":null,
       "visibility":null,
       "width":null
      }
     },
     "93b073cb5b0a480a9799a71876ba3ca8":{
      "model_name":"ProgressStyleModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module":"@jupyter-widgets\/controls",
       "_model_module_version":"1.5.0",
       "_model_name":"ProgressStyleModel",
       "_view_count":null,
       "_view_module":"@jupyter-widgets\/base",
       "_view_module_version":"1.2.0",
       "_view_name":"StyleView",
       "bar_color":null,
       "description_width":""
      }
     },
     "04c5e9a11a214e16842aa039244d1187":{
      "model_name":"FloatProgressModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_dom_classes":[
        
       ],
       "_model_module":"@jupyter-widgets\/controls",
       "_model_module_version":"1.5.0",
       "_model_name":"FloatProgressModel",
       "_view_count":null,
       "_view_module":"@jupyter-widgets\/controls",
       "_view_module_version":"1.5.0",
       "_view_name":"ProgressView",
       "bar_style":"success",
       "description":"",
       "description_tooltip":null,
       "layout":"IPY_MODEL_8e9699b4b67145cba596d6cd2130376f",
       "max":26970,
       "min":0,
       "orientation":"horizontal",
       "style":"IPY_MODEL_93b073cb5b0a480a9799a71876ba3ca8",
       "value":26970
      }
     },
     "ee33201e68284f2ca50a0c69c7e99209":{
      "model_name":"LayoutModel",
      "model_module":"@jupyter-widgets\/base",
      "model_module_version":"1.2.0",
      "state":{
       "_model_module":"@jupyter-widgets\/base",
       "_model_module_version":"1.2.0",
       "_model_name":"LayoutModel",
       "_view_count":null,
       "_view_module":"@jupyter-widgets\/base",
       "_view_module_version":"1.2.0",
       "_view_name":"LayoutView",
       "align_content":null,
       "align_items":null,
       "align_self":null,
       "border":null,
       "bottom":null,
       "display":null,
       "flex":null,
       "flex_flow":null,
       "grid_area":null,
       "grid_auto_columns":null,
       "grid_auto_flow":null,
       "grid_auto_rows":null,
       "grid_column":null,
       "grid_gap":null,
       "grid_row":null,
       "grid_template_areas":null,
       "grid_template_columns":null,
       "grid_template_rows":null,
       "height":null,
       "justify_content":null,
       "justify_items":null,
       "left":null,
       "margin":null,
       "max_height":null,
       "max_width":null,
       "min_height":null,
       "min_width":null,
       "object_fit":null,
       "object_position":null,
       "order":null,
       "overflow":null,
       "overflow_x":null,
       "overflow_y":null,
       "padding":null,
       "right":null,
       "top":null,
       "visibility":null,
       "width":null
      }
     },
     "7e4cd648d2524362ade5fa3072bd44ef":{
      "model_name":"DescriptionStyleModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module":"@jupyter-widgets\/controls",
       "_model_module_version":"1.5.0",
       "_model_name":"DescriptionStyleModel",
       "_view_count":null,
       "_view_module":"@jupyter-widgets\/base",
       "_view_module_version":"1.2.0",
       "_view_name":"StyleView",
       "description_width":""
      }
     },
     "0b7522f1555e45698d69b38502049e91":{
      "model_name":"HTMLModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_dom_classes":[
        
       ],
       "_model_module":"@jupyter-widgets\/controls",
       "_model_module_version":"1.5.0",
       "_model_name":"HTMLModel",
       "_view_count":null,
       "_view_module":"@jupyter-widgets\/controls",
       "_view_module_version":"1.5.0",
       "_view_name":"HTMLView",
       "description":"",
       "description_tooltip":null,
       "layout":"IPY_MODEL_ee33201e68284f2ca50a0c69c7e99209",
       "placeholder":"​",
       "style":"IPY_MODEL_7e4cd648d2524362ade5fa3072bd44ef",
       "value":"100%",
       "disabled":false
      }
     },
     "279077bfd51647588d9980f739a80ca1":{
      "model_name":"LayoutModel",
      "model_module":"@jupyter-widgets\/base",
      "model_module_version":"1.2.0",
      "state":{
       "_model_module":"@jupyter-widgets\/base",
       "_model_module_version":"1.2.0",
       "_model_name":"LayoutModel",
       "_view_count":null,
       "_view_module":"@jupyter-widgets\/base",
       "_view_module_version":"1.2.0",
       "_view_name":"LayoutView",
       "align_content":null,
       "align_items":null,
       "align_self":null,
       "border":null,
       "bottom":null,
       "display":null,
       "flex":null,
       "flex_flow":null,
       "grid_area":null,
       "grid_auto_columns":null,
       "grid_auto_flow":null,
       "grid_auto_rows":null,
       "grid_column":null,
       "grid_gap":null,
       "grid_row":null,
       "grid_template_areas":null,
       "grid_template_columns":null,
       "grid_template_rows":null,
       "height":null,
       "justify_content":null,
       "justify_items":null,
       "left":null,
       "margin":null,
       "max_height":null,
       "max_width":null,
       "min_height":null,
       "min_width":null,
       "object_fit":null,
       "object_position":null,
       "order":null,
       "overflow":null,
       "overflow_x":null,
       "overflow_y":null,
       "padding":null,
       "right":null,
       "top":null,
       "visibility":null,
       "width":null
      }
     },
     "8873ed7f90c744b1a48406d6a369f1f4":{
      "model_name":"DescriptionStyleModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_model_module":"@jupyter-widgets\/controls",
       "_model_module_version":"1.5.0",
       "_model_name":"DescriptionStyleModel",
       "_view_count":null,
       "_view_module":"@jupyter-widgets\/base",
       "_view_module_version":"1.2.0",
       "_view_name":"StyleView",
       "description_width":""
      }
     },
     "406c062d3aa24ad8b98473bc4d645254":{
      "model_name":"HTMLModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_dom_classes":[
        
       ],
       "_model_module":"@jupyter-widgets\/controls",
       "_model_module_version":"1.5.0",
       "_model_name":"HTMLModel",
       "_view_count":null,
       "_view_module":"@jupyter-widgets\/controls",
       "_view_module_version":"1.5.0",
       "_view_name":"HTMLView",
       "description":"",
       "description_tooltip":null,
       "layout":"IPY_MODEL_279077bfd51647588d9980f739a80ca1",
       "placeholder":"​",
       "style":"IPY_MODEL_8873ed7f90c744b1a48406d6a369f1f4",
       "value":" 26970\/26970 [00:24&lt;00:00, 1388.91it\/s]",
       "disabled":false
      }
     },
     "80c9bac3e0ed407a8a80089bdb724f0c":{
      "model_name":"LayoutModel",
      "model_module":"@jupyter-widgets\/base",
      "model_module_version":"1.2.0",
      "state":{
       "_model_module":"@jupyter-widgets\/base",
       "_model_module_version":"1.2.0",
       "_model_name":"LayoutModel",
       "_view_count":null,
       "_view_module":"@jupyter-widgets\/base",
       "_view_module_version":"1.2.0",
       "_view_name":"LayoutView",
       "align_content":null,
       "align_items":null,
       "align_self":null,
       "border":null,
       "bottom":null,
       "display":null,
       "flex":null,
       "flex_flow":null,
       "grid_area":null,
       "grid_auto_columns":null,
       "grid_auto_flow":null,
       "grid_auto_rows":null,
       "grid_column":null,
       "grid_gap":null,
       "grid_row":null,
       "grid_template_areas":null,
       "grid_template_columns":null,
       "grid_template_rows":null,
       "height":null,
       "justify_content":null,
       "justify_items":null,
       "left":null,
       "margin":null,
       "max_height":null,
       "max_width":null,
       "min_height":null,
       "min_width":null,
       "object_fit":null,
       "object_position":null,
       "order":null,
       "overflow":null,
       "overflow_x":null,
       "overflow_y":null,
       "padding":null,
       "right":null,
       "top":null,
       "visibility":null,
       "width":null
      }
     },
     "8a582d357493472481ed22edf1dd2c6e":{
      "model_name":"HBoxModel",
      "model_module":"@jupyter-widgets\/controls",
      "model_module_version":"1.5.0",
      "state":{
       "_dom_classes":[
        
       ],
       "_model_module":"@jupyter-widgets\/controls",
       "_model_module_version":"1.5.0",
       "_model_name":"HBoxModel",
       "_view_count":null,
       "_view_module":"@jupyter-widgets\/controls",
       "_view_module_version":"1.5.0",
       "_view_name":"HBoxView",
       "box_style":"",
       "children":[
        "IPY_MODEL_0b7522f1555e45698d69b38502049e91",
        "IPY_MODEL_04c5e9a11a214e16842aa039244d1187",
        "IPY_MODEL_406c062d3aa24ad8b98473bc4d645254"
       ],
       "layout":"IPY_MODEL_80c9bac3e0ed407a8a80089bdb724f0c"
      }
     }
    }
   }
  },
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "version":1,
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    
   ]
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}